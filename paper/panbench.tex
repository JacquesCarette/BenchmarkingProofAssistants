\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate,anonymous]{lipics-v2021}
%This is a template for producing LIPIcs articles. 
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling autoref support, use "autoref"
%for anonymousing the authors (e.g. for double-blind review), add "anonymous"
%for enabling thm-restate support, use "thm-restate"
%for producing a PDF according the PDF/A standard, add "pdfa"

%\pdfoutput=1 %uncomment to ensure pdflatex processing (mandatatory e.g. to submit to arXiv)
%\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\usepackage{panbench}

\title{\panbench: A Comparative Benchmarking Tool for Dependently-Typed Languages}

\author{Reed Mullanix}{Department of Computing and Software, McMaster University, Canada}{mullanir@mcmaster.ca}{https://orcid.org/0000-0002-7970-4961}{}
\author{Jacques Carette}{Department of Computing and Software, McMaster University, Canada}{carette@mcmaster.ca}{https://orcid.org/0000-0001-8993-9804}{}

\authorrunning{R. Mullanix and J. Carette}

\Copyright{Reed Mullanix and Jacques Carette}


\ccsdesc[500]{Mathematics of computing~Mathematical software performance}

\keywords{Benchmarking, dependent types, testing}

\category{} %optional, e.g. invited paper

\relatedversion{} %optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversiondetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93]{Classification (e.g. Full Version, Extended Version, Previous Version}{URL to related version} %linktext and cite are optional

%\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
%\supplementdetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93, subcategory={Description, Subcategory}, swhid={Software Heritage Identifier}]{General Classification (e.g. Software, Dataset, Model, ...)}{URL to related version} %linktext, cite, and subcategory are optional

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

%\nolinenumbers %uncomment to disable line numbering

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

\begin{abstract}
We benchmark four proof assistants (\agda, \idris, \lean and \rocq) through a
single test suite. We focus our benchmarks on the basic features that all
systems based on a similar foundations (dependent type theory) have in common.
We do this by creating an ``over language'' in which to express all the
information we need to be able to output \emph{correct and idiomatic syntax}
for each of our targets. Our benchmarks further focus on ``basic engineering''
of these systems: how do they handle long identifiers, long lines, large
records, large data declarations, and so on.

Our benchmarks reveals both flaws and successes in all systems. We give a
thorough analysis of the results.

We also detail the design of our extensible system. It is designed so that
additional tests and additional system versions can easily be added. A side
effect of this work is a better understanding of the common abstract syntactic
structures of all four systems.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Production-grade implementations of dependently typed programming languages
are complicated pieces of software that feature many intricate and potentially
expensive algorithms. As such, large amounts of engineering effort has been dedicated
to optimizing these components. Unfortunately, engineering time is a finite
resource, which entails that other parts of these systems
get comparatively less attention. This often results in easy-to-miss performance problems:
a proof assistant developer told us that a na\"ive \(O(n^{2})\)
fresh name generation algorithm used for pretty-printing resulted in 100x slowdowns
in some pathological cases.

Thus the need for a benchmarking suite for these simpler components.  Moreover, such
a benchmarking suite would also be valuable for developers of new dependently typed
languages, as it is much easier to optimize with a performance goal in mind. This
is an instance of the classic \(m \times n\) language tooling problem: constructing
a suite of \(m\) benchmarks for \(n\) languages directly requires a quadratic
amount of work up.

Like most \(m \times n\) tooling problems, the solution is to introduce
a mediating tool. In our case, we ought to write all of the benchmarks
in an intermediate language, and then translate that intermediate language to
the target languages in question. There are existing languages like Dedukti~\cite{Dedukti:2023}
or Informath~\cite{Informath:2026}
that attempt to act as an intermediary between proof assistants, but these
tools typically focus on translating the \emph{content} of proofs, not exact
syntactic structure. To fill this gap, we have created the \panbench system, which consists of:

\begin{enumerate}
\item An extensible embedded Haskell DSL that encodes the concrete syntax of a
  typical dependently typed language.
\item A series of compilers for that DSL to \agda~\cite{AgdaDevelopers:Agda}, \idris~\cite{Brady:2021}, \lean~\cite{Lean4:2021}, and \rocq~\cite{Rocq:2026}.
\item A benchmarking harness that can perform sandboxed builds of
  multiple versions of \agda, \idris, \lean, and \rocq.
\item An incremental build system that can produce benchmarking reports as
  static HTML files or PGF plots\footnote{All plots in this paper were produced directly by \panbench.}.
\end{enumerate}

% Annoying, we can't use the 'enumitem' package, so we have to do inline enumerations ourselves.
Concretely, we see our contributions as 
1) concrete benchmarks and analysis thereof,
2) the supporting extensible infrastructure, and
3) the design of the embedded \panbench DSL for system-agnostic tests.
All three required extensive work, including many design iterations, before settling on
what is here.

Our paper is structured as follows. We detail our methodology
(Section~\ref{sec:method}), both for how we came up with our tests and the experimental setup.
Actual results (Section~\ref{sec:results}) are given and analyzed. We then take a step back
and look at the results more globally, and speculate on some of the reasons for the weaknesses
we found. Section~\ref{sec:infrastructure} documents the engineering of the infrastructure parts
of \panbench (build system, test harness and report generator). The \emph{language framework}
and its design is described in Section~\ref{sec:design}. Lastly we conclude.

\section{Methodology}
\label{sec:method}

To perform reliable benchmarking, we need tooling and we need to design
a test suite. For reproducibility, we also need to document the actual
experimental setup. The tooling will be described later,
now we focus on the design of the test suite and the experimental setup.

\begin{tabular}{l|l||l|l}
\textbf{Category} & \textbf{Details} & \textbf{Category} & \textbf{Details} \\ \hline
Syntax & Newlines            & Datatypes  & Parameters \\
       & Parentheses         &            & Indices \\
Names  & Datatype            &            & Params + Indices \\
       & Data constructor    &            & Constructors \\
       & Definition          & Records & Fields \\
       & Definition lhs      &         & Parameters \\
       & Definition rhs      & Dependency & Record Fields\\
       & Lambda              &            & Datatypes Parameters\\
       & Pi                  &            & Definitions chains \\
       & Record              &            & Record chains \\
       & Record constructor  & Nesting & Id chain \\
       & Record field        &         & Id chain \(\lambda\) \\
Binders & Lambda             &         & Let \\
        & Implicits          &         & Let addition \\
Misc    & Postulates         &         & Let functions \\
Conversion & Addition        &         & \\
\end{tabular}

\subsection{Designing tests}

Let us assume that we possess the following tools:
a) a single language of tests,
b) a means to translate these tests to each of our four languages,
b) a reproducible test harness, and
d) a stable installation of the four languages.
How would we design actual tests for common features?

As we said before, our aim here is to test ``basic engineering''.  For example,
how does each system handle giant files (e.g.: one million empty lines),
large names (thousands of characters long) in each syntactic category,
large numbers of fields in records, constructors in algebraic types, large
numbers of parameters or indices, long dependency chains, and so on. We
divide our tests into the following categories:
\begin{enumerate}
\item basic syntactic capabilities
\item long names in simple structures
\item large simple structures (data, record)
\item handling of nesting and dependency
\item conversion, postulates
\end{enumerate}
\noindent where we vary the ``size'' of each.

At a higher level, we asked ourselves the question: for each aspect of a
dependently typed language (lexical structure, grammatical structure,
fundamental features of dependent languages), what could be made to ``scale''?
The only features of interest remained the ones that were clearly features of
all four systems.

Note that we are \emph{not} trying to create ``intrinsically interesting'' tests,
but rather we want them to be \emph{revealing} with respect to the need for
basic infrastructure improvements.

For lack of space, we cannot show samples of all the tests, but we show
just a few that should be \emph{illustrative} of the rest\footnote{Very minor
edits made to \panbench output in the listings to fit the page width.}. The source of
all tests can be found in the accompanying material. All tests below
are from the snapshot of our ``golden'' test suite, uniformly run with a
size parameter of \(5\).

\begin{minipage}{0.50\textwidth}
\begin{lstlisting}[style=agda,caption=Nesting: Id chain (Agda),label=code:IdChain]
module IdChain where

f : {a : Set} (x : a) -> a
f x = x

test : {a : Set} -> a -> a
test = f f f f f f
\end{lstlisting}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\begin{lstlisting}[style=lean,caption=Nesting: Let add (Lean),label=code:LetAdd]
def n : Nat :=
  let x0 := 1
  let x1 := x0 + x0
  let x2 := x1 + x1
  let x3 := x2 + x2
  let x4 := x3 + x3
  x4
\end{lstlisting}
\end{minipage}

\begin{lstlisting}[style=idris,caption=Binders: Lambda (Idris 2), label=code:LargeLambda]
module Main

const5 : {A : Type} -> {B0, B1, B2, B3, B4, B5 : Type} ->
  A -> B0 -> B1 -> B2 -> B3 -> B4 -> B5 -> A
const5 = \a, b0, b1, b2, b3, b4, b5 => a
\end{lstlisting}

\begin{lstlisting}[style=rocq,caption=Dependency: Record Telescope (Rocq), label=code:RecTele]
Module RecordTelescope.

Record Telescope (U : Type) (El : U -> Type) : Type := Tele
  { a0 : U
  ; a1 : forall (x0 : El a0), U
  ; a2 : forall (x0 : El a0) (x1 : El (a1 x0)), U
  ; a3 : forall (x0 : El a0) (x1 : El (a1 x0)) (x2 : El (a2 x0 x1))
    , U
  ; a4 : forall (x0 : El a0) (x1 : El (a1 x0)) (x2 : El (a2 x0 x1)) 
    (x3 : El (a3 x0 x1 x2)), U
  ; a5 : forall (x0 : El a0) (x1 : El (a1 x0)) (x2 : El (a2 x0 x1))
    (x3 : El (a3 x0 x1 x2)) (x4 : El (a4 x0 x1 x2 x3)), U
  }.

Arguments Tele {U} {El} _ _ _ _ _ _.

End RecordTelescope.
\end{lstlisting}

\subsection{Experimental setup}

All tests are run on a dedicated desktop machine running on
NixOS 25.11. The CPU is an Intel i7-2600K with 1MB L2 cache, 8MB L3
cache, running at 3.4 Ghz, equipped with 24 Gigs of DDR3 memory. This
box has no SSD drive, but this should only affect system time.
When tests are run, no other (human) activity happens on the machine.

All tests are run with a time limit of \(60\) seconds. We also tried to
use memory limits but, alas, Linux no longer reliably supports these.
We ran all systems in their default configuration.

\section{Results}
\label{sec:results}

Given that our test suite has \(31\) tests, each of which produces \(3\)
different graphs, we have no room to display all \(93\) resulting
graphs. We thus choose results that
appear to be the most ``interesting''. Furthermore, other than in
Table~\ref{tab:start-up}, we will not show the \emph{System Time} as it
correlates very strongly with memory use for our particular test suite.

\begin{figure}[htb]
\begin{tabular}{l|r|r|r}
\textbf{System} & \textbf{User Time (s)} & \textbf{System Time (s)} & \textbf{Max RSS (MB)} \\ \hline
\agda  & 0.02 (0.002) & 0.01 (0.001) & 64 (0.1) \\
\idris & 0.58 (0.007) & 0.10 (0.007) & 248 (0.1) \\
\lean  & 0.14 (0.005) & 0.04 (0.005) & 307 (0.6) \\
\rocq  & 0.05 (0.004) & 0.03 (0.003) & 95 (0.05) \\
\end{tabular}
\caption{Start-up time and memory, mean with standard deviation in parentheses.}
\label{tab:start-up}
\end{figure}

Start-up time and memory use varies wildly: from a super-fast (0.02s) and
slim (64 MB) \agda to a 29 times slower \idris and 4.8 times memory consumer in \lean.
It is worth recalling that 0.1 seconds is the ``instant'' threshold.

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/Newlines/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/Newlines/rss}}
\end{minipage}
\caption{Blank lines}
\label{fig:Newline}
\end{figure}

A file with a header and a million blank lines is not intrinsically
interesting.  It is however very \emph{revealing}: we see
(Figure~\ref{fig:Newline}) that it takes \lean and \rocq no noticeable time to
deal with that, while already 100K lines causes both \agda and \idris to slow
down and consume significantly more memory (6.3Gigabytes for 10 million lines
in \idris's case).  Estimating the run-time for \agda and \idris from the slope
of the graphs, we get that both are linear.

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/LongNameDataType/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/LongNameDataType/rss}}
\end{minipage}
\caption{Long names (datatypes)}
\label{fig:LongNameDataType}
\end{figure}

What about \emph{long identifiers}? Figure~\ref{fig:LongNameDataType} shows what happens when we
use increasingly long names for data types; other ``long names'', such as for constructors,
field names of records, etc, show similar behaviour. Unlike for blank lines, all systems show
an eventual increase in time and memory use, with \agda starting earlier than others. What is most 
interesting is that \lean barely takes any more memory, even for extremely long (4 million characters)
identifiers. \agda's time here too indicates it is linear in the number of characters.

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/LargeSimpleDatatype/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/LargeSimpleDatatype/rss}}
\end{minipage}
\caption{Enumerations}
\label{fig:Enum}
\end{figure}

Figure~\ref{fig:Enum} tests simple data types (enumerations) with short constructor names.
What is remarkable here is \idris: even at \(2^{11}\) constructors, it takes no appreciable time
or memory beyond start-up.

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/LargeLambda/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/LargeLambda/rss}}
\end{minipage}
\caption{Lambda term with many variables}
\label{fig:Lambda}
\end{figure}

Figure~\ref{fig:Lambda}, a test for a single lambda term that uses its first
variable but has $n$ other (unused) variables also declared (see
Listing~\ref{code:LargeLambda}. This seems to be a ``torture'' test where
systems perform very well until they hit a wall and suddenly time out. It
appears that the underlying problem is using an enormous amount of memory.

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/RecordTelescope/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/RecordTelescope/rss}}
\end{minipage}
\caption{Record with increasingly dependent fields}
\label{fig:RecTele}
\end{figure}

Figure~\ref{fig:RecTele} is a test for ``very dependent records'', i.e. a record
where every later field depends on all previous ones, as shown in Listing~\ref{code:RecTele}.
Of course, the code size itself is quadratic in the number of such fields.  As expected,
this rapidly takes quite a lot of time; less expected is the memory usage also goes up
very significantly. This particular test likely needs finer sampling (and maybe larger
timeouts) to understand the behaviour of each system.

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/IdChain/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/IdChain/rss}}
\end{minipage}
\caption{Chain of calls to identity function}
\label{fig:IdChain}
\end{figure}

Figure~\ref{fig:IdChain}, corresponding to Listing~\ref{code:IdChain}, nested calls to
an identity function, shows even more
extreme behaviour: basically instantaneous until memory explosion and time out. This is
well-known to be a problem for type systems based on Hindley-Milner inference, but it is
less clear that it ought to be a weakness for bidirectional typing as well. Closer
sampling (not shown) does not change this picture.

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/NestedLetAdditions/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/NestedLetAdditions/rss}}
\end{minipage}
\caption{Nested let bindings, simple addition on rhs}
\label{fig:NestedLetAdd}
\end{figure}

Figure~\ref{fig:NestedLetAdd}, corresponding to Listing~\ref{code:LetAdd}, nested lets
doing very simple arithmetic. The contrast is remarkable: \agda and \idris time out already
at size \(2^5\), \lean takes an increasing amount of time, while \rocq takes no time nor any
memory! Note the enormous amount of memory taken by \agda when it times out.

\begin{minipage}{0.49\textwidth}
\begin{lstlisting}[style=idris,caption=Conversion: Addition (Idris),label=code:ConvAdd]
conv : 5 + 5 + 5 + 5 + 5 + 0 = 25
conv = Refl
\end{lstlisting}
\end{minipage}

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/ConversionAddition/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/ConversionAddition/rss}}
\end{minipage}
\caption{Conversion for Natural Number Addition}
\label{fig:ConvAdd}
\end{figure}

Figure~\ref{fig:ConvAdd}, corresponding to Listing~\ref{code:ConvAdd}, does simple
natural number arithmetic and ensures the correct result is obtained. Here the roles
are inverted: \agda, \idris, and \lean take no time while \rocq takes an increasing amount
of time until timeout.


% Order:
% - very basic syntactic
% - long names of simple structures
% - nested structures
% - dependency
% - conversion
%
% Meta:
% - "get worse" is when the curve is not a straight line; these are log-log plots,
%     so that indicates exponential behaviour
% - system time variance hard to explain, though seems largely related to memory
%
% --------------
% Empty
% Meta: x-axis should not be log but uniform
% Comment: this is basically measuring start-up time, which varies a lot
%  - agda is very fast, lean is a bit slow, idris2 quite slow
%  - on max RSS, lean and idris2 are very greedy
% 
% Newlines
%
% - rocq and lean survive and only get measurably worse at n = 10^7
% - idris2 times out at the end
% - agda and idris2 get worse already at 10,000
% - memory goes up for all, but *much* worse for Agda/Idris2
% 
% Parens
%
% - there must a bug in lean's handling of parents: it is fine until 256 then dies
% - idris dies at 2^16, rocq at 2^14, while agda survives 2^16
% - (should compute slope of that straight line in idris2 and agda memory usage)
% 
% --------------
% 
% Meta: all LongName are fairly similar (good!) but still not entirely
%
% LongNameDatatype
%
% - idris2 handles quite smoothly until giant length
% - agda blows up much earlier
% - unknown why super-short length is (significantly) worse
% - lean's memory use is great, rest blow up, agda worst
% 
% LongNameDatatypeConstructor
% 
% - fairly similar (to above) except idris2 much more stable and rocq's memory use stabilizes
%
% LongNameDefinition
% 
% - similar to above but irdris2 blows up (didn't above(
%
% LongNameDefinitionLhs
%
% - idris2 now does not really blow up (though it starts to consume memory)
% - memory
% 
% LongNameDefinitionRhs
% 
% - same as above
%
% LongNameLambda
% 
% - long name lambda and Pi are 'the same', which is good (except for the blow up
%  of Agda and Idris2)
%
% LongNamePi
% 
% - see above
%
% - next 3 again follow same pattern
% LongNameRecord
% 
% LongNameRecordConstructor
% 
% LongNameRecordField
% 
% --------------
%
% DatatypeParameters
%
% - impressive performance (minus start-up) from idris2 and rocq
% - agda and lean "blow up" around size 30
% 
% LargeIndexedDatatype
% 
% - all systems get worse, and all but rocq time out at n=256
% - Idris goes from its usual slow to timing out entirely with no transition
%
% LargeIndexedParameterisedDatatype
% 
% - idris2 rocks this one, with agda a close second
% - rocq explodes wrt memory, and lean on time
%
% LargeLambda
%
% - idris2 plateau unexplained; decrease in memory even more so
% - lean's spike?
% - agda suddenly blows up (seems to be memory related)
% 
% LargeSimpleDatatype
% 
% - idris2 does extremely well here
% - all others (slowly) get worse 
%
% LargeSimpleRecord
% 
% - idris2 does very well until the very end and blows up
% - agda and rocq handle very well
% - lean takes an increasingly large amount of time while rocq's memory blows up
% 
% ManyImplicits
% 
% - extremely weird idris2 behaviour: flat then spike
% - rocq and agda time curve up very gently, lean more seriously
% - rocq and agda start to consume more memory (from a very low level)
%
% * go up one more notch?
%
% Postulates
%
% - idris2 gets worse quickly, times out at 2^15
% - same for lean, times out earlier at 2^13
% - rocq and agda survive but seems to be in O(n^2) ? territory?
% 
% RecordParameters
% 
% - only rocq survives, rest die at 2^9
% - not clear what causes the problem
%
% SequentialDefinitions
% 
% - rocq clearly has worse definition (different slope), but memory behaviour is odd
% 
% SequentialSimpleRecords
%
% - idris2 seems to handle this extremely well
% - rest eventually starts to goblle memory
% - lean really does not handle this well at all
% 
% SimpleDataDefinitions
%
% - lean times out
% - same slope for rocq/agda and likely lean; not clear for idris2
% ----------------
%
% LargeDependentRecord
%
% all systems get worse, following a similar path, 
% rest time out (lean at n=128, rest at n=256), only rocq survives
% memory use goes way way up
% 
% SequentialDependentRecords
%
% - lean times out quickly, Idris eventually
% - none seem to "level out" to straight lines
% 
% ----------------
% 
% NestedLet
% 
% - idris2 times out(n=1024), as does lean (n=512), rest do fine
%
% * another notch?
%
% NestedLetAdditions
% 
% - lots of tiem outs (agda, idris2 already after n=16), lean gets worse but survives,
%   rocq kills it
%
% NestedLetFunctions
% 
% - all systems blow up, at different points, as well as consume a lot of memory
% - lean, idris2 time out at n=128, agda at n=256, rocq survives
%
% IdChain
%
% - all systems blow up and time out (seems driven by memory use) between 8 and 16
%    except for lean, which uses no more memory at 32 and just a touch more time
% 
% ConversionAddition
% - Agda's higher time for the small case is unexplained
% - rocq is the only one that "blows up" with size
%

\section{Discussion}
\label{sec:discuss}

The previous section analyzed the results themselves. Here we speculate on why
the results may be as they are. We have not (yet) verified any of our suspicions.

\subsection{\agda}

Most the results for \agda follow a general pattern: it is consistently the
fastest and most memory efficient for small inputs, but has a couple of edge
cases where it really struggles. Notably, \agda performs rather poorly on tests
that put heavy pressure on the parser. This poor performance can be explained
by the use of Haskell's \lstinline|String| inside of Agda's lexer, which is a
known performance pitfall.

\agda also struggles to typecheck let-bindings that introduce some non-trivial
sharing. This is an unfortunate consequence of \agda's choice to inline let
bindings during elaboration, which can easily result in exponential blowups
if sharing is lost.

\subsection{\idris}

Unfortunately, \idris struggles on a lot of these tests. It has the highest startup time
of any systems we benchmarked, coming in at over half a second. We suspect the choice to
use scheme as a runtime, which incurs a high interpreter
startup cost.

Like \agda, \idris struggles on benchmarks that stress-test the parser. Notably, it
is the only system that times out when trying to parse files containing \(10^{7}\) newlines,
and consumes over 6 gigabytes while doing so. We suspect
\idris's \texttt{Text.Lexer} library.
It also struggles with let-bindings, and times out when trying to elaborate a linear sequence of 1024
let bindings. This does not appear to have the same root cause as \agda's poor performance,
and further investigation is required.

However, there are some bright spots. \idris performs \emph{exceptionally} well on all tests
involving elaboration of datatypes and records once the startup time is accounted for.

\subsection{\lean}

Initially, we expected that \lean would perform well.
We were surprised by the number of time outs and hard crashes
we encountered. Many tests involving records and datatypes hit hard limits
for the number of fields, indices, or constructors. It also seems to struggle
on elaboration tasks that involve large numbers of names: we suspect that
this could be due to using a locally-nameless
representation~\cite{Chargueraud:2012} internally.

On the bright side, \lean does manage to elaborate the nested addition test,
though it does seem to exhibit some exponential behaviour while doing so.
It also is able to easily handle almost all of the parsing tests with ease,
though it does stack overflow after 512 nested parenthesis. It was also the
only system that was able to handle checking 32 iterated applications of the
identity function, and did so with ease, taking approximately 0.25 seconds and
a negligible amount of additional memory.

\subsection{\rocq}

Out of all of the systems we measured, \rocq was by far and away the winner.
On most tests, it typically came in first or second place. Notably, it
was the only system that was able to handle the nested addition test without
exhibiting some sort of exponential blow-up. It also consistently outperforms
other systems on parsing-heavy tasks, though it does stack overflow when
presented with 16384 nested parenthesis.

However, there were still some surprises. \rocq was the only system that
exhibited linear runtime on the addition conversion test: all other systems
managed to run in essentially constant time. It also struggled to handle
files that contained large numbers of very simple definitions, and took nearly
45 seconds to typecheck 4096 of them. We suspect that both of these may have
to do with a sub-optimal approach to checking natural numbers.

\section{Infrastructure}
\label{sec:infrastructure}

One of the major goals of \panbench is to make performance analysis as
low-cost as possible for language developers. Meeting this goal requires
a large amount of supporting infrastructure: simply generating benchmarks
is not very useful if you cannot reliably run them nor analyze their outcomes.
We concluded that for ease of use and adoption, reliability and
reproducibility, a language benchmarking system should:

\begin{enumerate}
\item Provide infrastructure for performing sandboxed builds of
  compilers from source.
\item Allow for multiple revisions of the same tool to be installed
  simultaneously.
\item Allow a single tool at a single version but with different build configurations.
\item Be able to be run locally on a developer's machine.
\item Present a declarative interface for creating benchmarking environments and running benchmarks.
\item Present performance results in a self-contained format that is easy to understand
  and share.
\end{enumerate}

Of these criteria, the first four present the largest engineering challenge, and are
tantamount to constructing a meta-build system that is able orchestrate \emph{other} build
systems. We approached the problem by constructing a bespoke content-addressed
system atop of Shake~\cite{Mitchell:2012}, which we discuss in section \ref{sec:builds}.
The final two criteria also presented some unforseen difficulties, which we detail in \ref{sec:reports}.

\subsection{The \panbench Build System}
\label{sec:builds}

As noted earlier, we strongly believe that any benchmarking system should provide infrastructure
for installing multiple versions of reproducibly built software. Initially, we intended to build
this atop of Nix~\cite{Dolstra:Nix}. This is seemingly a perfect fit;
after all, Nix was designed to facilitate almost exactly this use-case. However, we did not
for the following reasons: 

\begin{enumerate}
\item Nix does not work natively on Windows. Performance problems
  are often operating system specific, so ruling out a popular OS
  seems unwise\footnote{Currently, \panbench does not support Windows, but this
  is an artifact of prioritization, and not a fundamental restriction.}.
\item Nix adds a barrier to adoption. Installing Nix is a somewhat
  invasive process, especially on MacOS\footnote{The situation is even worse on x86-64 Macs, which most Nix installers
  simply do not support.}.
We believe that it is unreasonable to ask developers to add users to their system and modify
their root directory to run a benchmarking tool, and strongly suspect that this would
hamper adoption.
\end{enumerate}

Thus we opted to create our own Nix-inspired build system based atop
Shake~\cite{Mitchell:2012}.  Shake works on Windows, and only requires
potential users to install a Haskell toolchain.

The details of content-addressed build systems are a deep topic unto themselves,
so we will only describe the key points. Systems like Nix use an
\emph{input-addressing} scheme, wherein the results of a build are
stored on disk prefixed by a hash of all build inputs. Crucially, this lets the build
system know where to store the result of the build before the build is run, which
avoids vicious cycles where the result of a build depends on its own hash. However,
most input-addressed systems also require that the hash of the inputs \emph{solely}
determines the output. On its face, this is a reasonable ask, but taking this
idea to its logical conclusion requires one to remove \emph{any} dependency
on the outer environment, which in turn forces one to re-package the entirety of the software
stack all the way down to \texttt{libc}. This is an admirable goal in its own right,
but is actually counterproductive for our use case: there is a very real chance
that we might end up benchmarking our sub-par repackaging of some obscure C dependency
four layers down the stack.

To avoid this cascading series of dependency issues, \panbench takes a hybrid approach,
wherein builds are input-addressed, but are allowed to also depend on the external
environment. Additionally, the results of builds are also hashed, and stored out-of-band
inside of a Shake database. This hash is used to invalidate downstream
results, and also act as a fingerprint to identify if two benchmarks were created from the
same binary. This enables a pay-as-you go approach to reproducibility, which we hope
will result in a lower adoption cost. Moreover, we can achieve fully reproducible builds
by using Nix as a meta-meta build system to compile \panbench: this is how we obtained
the results presented in Section \ref{sec:results}.

\subsection{Running Benchmarks and Generating Reports}
\label{sec:reports}

As noted earlier, we believe that benchmarking
tools should present a \emph{declarative} interface for writing not just single
benchmarking cases, but entire benchmarking suites and their corresponding environments.
\panbench accomplishes this by \emph{also} implementing the benchmark execution framework
atop Shake. This lets us easily integrate the process of tool installation with environment
setup, but introduces its own set of engineering challenges.

The crux of the problem is that performance tests are extremely sensitive to the current
load on the system. This is at odds with one of the goals of a build system,
which is to try to complete a build as fast as possible by using all available
resources. This can be avoided via careful use of locks, but we are then faced
with another, larger problem. Haskell is a garbage collected language, and
running the GC can put a pretty heavy load on the system. Moreover, the GHC
runtime system is very well engineered, and is free to run the garbage
collector inside of \texttt{safe} FFI calls, and waiting for process completion
is marked as \texttt{safe}.

To work around this, we opt to eschew existing process interaction
libraries, and implement the benchmark spawning code in C\footnote{This is why
\panbench does not currently support Windows.}.
This lets us take the rather extreme step of linking against the GHC runtime system
so that we can call \texttt{rts_pause}, which pauses all other Haskell threads and GC
sweeps until \texttt{rts_resume} is called.

Initially, we thought that this was the only concern that would arise
by tightly integrating the build system with the benchmark executor.
However, our initial benchmarks on Linux systems displayed some very strange
behaviour, wherein the max resident set size reported by \texttt{getrusage}
and \texttt{wait4} would consistently report a reading of approximately
2 gigabytes halfway through a full benchmarking suite.
We discovered the Linux preserves resource usage statistics across calls to \texttt{execve}.
Consequentially, this means that we are unable to measure any max RSS that
is lower than max RSS usage of \panbench itself. Luckily, our lowest baseline is Agda at 64 megs,
and we managed to get the memory usage of \panbench itself down to approximately 10 megs via
some careful optimization and GC tuning.

Currently, the statistics that \panbench gathers can then be rendered into standalone HTML
files with \texttt{vega-lite} plots, or into \TeX{} files containing PGF plots. We intend to add more
visualization and statistic analysis tools as the need arises.

\section{The Design of \panbench}
\label{sec:design}

At its core, \panbench is a tool for producing grammatically well-formed concrete
syntax across multiple different languages. Crucially, \panbench does \emph{not}
require that the syntax produced is well-typed or even well-scoped: if it did,
then it would be impossible to benchmark how systems perform when they encounter
errors. This seemingly places \panbench in stark contrast with other software tools for
working with the meta-theoretic properties of type systems, which are typically
concerned only with well-typed terms.

However, the core task of \panbench is not that different from that of
a logical framework~\cite{Harper-Honsell-Plotkin:1993}: Both
exist to manipulate judgements, inference rules, and derivations:
\panbench just works with \emph{grammatical} judgements and production
rules rather than typing judgments and inference rules. In this sense
\panbench is a \emph{grammatical} framework\footnote{Not to be
  confused with \emph{the} Grammatical Framework~\cite{Ranta:2011},
  which aims to be a more natural-language focused meta-framework for
  implementing and translating between grammars.  } rather than a
logical one.

This similarity let us build \panbench atop well-understood design 
principles. In particular, a mechanized logical framework typically
consists of two layers:

\begin{enumerate}
\item A layer for defining judgements \`{a} la relations.
\item A logic programming layer for synthesizing derivations.
\end{enumerate}

To use a logical framework, one first encodes a language by laying out all of the
judgements. Then, one needs to prove an adequacy theorem that
shows that their encoding of the judgements actually aligns with the language. However,
mechanizing this adequacy proof would require a staged third layer consisting of a more
traditional proof assistant.

Transposing this skeleton design to grammatical constructs
we will also obtain three layers:

\begin{enumerate}
\item A layer for defining grammars as relations.
\item A logic programming layer for synthesizing derivations.
\item A staged functional programming layer for proving ``adequacy'' results.
\end{enumerate}

In this case, an adequacy result for a given language \(\mathcal{L}\) is a constructive proof that
all grammatical derivations written within the framework can be expressed within the
concrete syntax of a language \(\mathcal{L}\). However, the computational content of such a proof
essentially amounts to a compiler written in the functional programming layer. Given that this
compiler outputs \emph{concrete syntax}, it is implemented as a pretty-printer.

\subsection{Implementing The Grammatical Framework}
\label{sec:framework-implementation}

Implementing a bespoke hybrid of a logic and functional programming language is
no small feat, and also requires prospective users to learn yet another single-purpose
tool. Luckily, there already exists a popular, industrial-grade hybrid logic/functional
programming language in wide use: GHC Haskell.

At first glance, Haskell does not contain a logic programming language. However,
if we enable enough GHC extensions, the typeclass system can be made \emph{just}
rich enough to encode a simply-typed logical framework. The key insight is that
we can encode each production rule using multi-parameter type classes with
a single method. Moreover, we can encode our constructive adequacy proofs for a
given set of production rules as instances that translate each of the productions
in the abstract grammar to productions in the syntax of an actual language.

As a concrete example, consider the grammar of the following simple imperative language.
\begin{grammar}
  <expr> := x | n | <expr> `+' <expr>
  
  <stmt> := <var> `=' <expr> | <stmt> `;' <stmt>
\end{grammar}

We can then encode this grammar with the following set of typeclasses:

\begin{lstlisting}[style=haskell,caption={An example tagless encoding.}]
class Var expr where
  var :: String -> expr

class Lit expr where
  lit :: Int -> expr

class Add expr where
  add :: expr -> expr -> expr

class Assign expr stmt where
  assign :: String -> expr -> stmt

class AndThen stmt where
  andThen :: stmt -> stmt -> stmt
\end{lstlisting}

This style of language encoding is typically known as the untyped variant of \emph{finally tagless}~\cite{Carette-Kiselyov-Shan:2009}.
However, our encoding is a slight refinement where
we restrict ourselves to a single class per production rule. Other
tagless encodings often use a class per syntactic category. This more fine-grained approach
allows us to encode grammatical constructs that are only supported by a subset of our target
grammars; see section \ref{sec:language-design} for examples.

Unfortunately, the encoding above has some serious ergonomic issues. In particular, expressions
like \lstinline|assign "x" (lit 4)| will result in an unsolved metavariable for \lstinline|expr|,
as there may be multiple choices of \lstinline|expr| to use when solving the \lstinline|Assign ?expr stmt|
constraint. Luckily, we can resolve ambiguities of this form through judicious use of functional dependencies~\cite{Jones:2000},
as demonstrated below.

\begin{lstlisting}[style=haskell, caption={A tagless encoding with functional dependencies.}]
class Assign expr stmt | stmt -> expr where
  assign :: String -> expr -> stmt

class While expr stmt | stmt -> expr where
  while :: expr -> stmt -> stmt
\end{lstlisting}

\subsection{Designing The Language}
\label{sec:language-design}

Now on to design our idealized abstract grammar. Our target languages
roughly agree on a subset of
the grammar of non-binding terms: the main sources of divergence are binding
forms and top-level definitions\footnote{As we shall see in section
\ref{sec:top-level}, the syntactical divergence of top-level definitions is
essentially about binders as well.}.
This is ultimately unsurprising: dependent type theories are fundamentally theories of binding and
substitution, so we would expect some variation in how our target languages present the core of
their underlying theories.

This presents an interesting language design problem. Our idealized grammar will need
to find ``syntactic abstractions'' common between our four target languages.  Additionally,
we would also like for our solution to be (reasonably) extensible. Finding the core
set of grammatical primitives to accomplish this task is surprisingly tricky, and requires
a close analysis of the fine structure of binding.

An analogy from linguistics might help contextualize the task: human languages vary
on whether they are subject-verb-object (SVO) or SOV, amongst many other possibilities. This
requires us to notice the syntactic categories ``subject'', ``object'' and ``verb''
that are common to all, and that explicit renderings
merely differ in the order. Similarly for singular and plural as modifiers. Our task is
similar.

\subsubsection{Binding Forms}
\label{sec:binding-form}

A binding form carries
much more information than just a variable name and a type. Moreover, this
extra information can have a large impact on typechecking performance, as is
the case with implicit/visible arguments. To further complicate matters,
languages often offer multiple syntactic options for writing the same binding
form, as is evidenced by the prevalence of multi-binders like \((x\ y\ z : A)
\to B\). Though such binding forms are often equivalent to their single-binder
counterparts as \emph{abstract} syntax, they may have different performance
characteristics, so we cannot simply lower them to a uniform single-binding
representation. To account for these variations, we have designed a
sub-language dedicated solely to binding forms. This language classifies the
various binding features along three separate axes: binding arity, binding
annotations, and binding modifiers.

Binding arities and annotations are relatively self-explanatory, and classify
the number of names bound, along with the type of annotation allowed. Our
target languages all have their binding arities falling into one of three
classes: \(n\)-ary, unary, or nullary. We can similarly characterise
annotations into three categories: required, optional, or forbidden.

This language of binders is encoded in the implementation as a single class \lstinline|Binder| that is parameterised
by higher-kinded types \lstinline|arity :: Type -> Type| and \lstinline|ann :: Type -> Type|, and provide standardized
types for all three binding arities and annotations.  

\begin{lstlisting}[style=haskell,caption={The core \panbench binder class.}]
class Binder arity nm ann tm cell | cell -> nm tm where
  binder :: arity nm -> ann tm -> cell
\end{lstlisting}
Production rules involving binding forms are encoded as classes parametric
over a notion of a binding cell.
\begin{lstlisting}[style=haskell, caption={The \panbench class for \(\Pi\)-types.}]
class Pi cell tm | tm -> cell where
  pi :: [cell] -> tm -> tm
\end{lstlisting}

Decoupling the grammar of binding forms from the grammar of binders themselves allows us
to be somewhat polymorphic over the language of binding forms when writing generators.
This in turn means that we can potentially re-use generators when extending \panbench with
new target grammars that may support only a subset of the binding features.

A binding
modifier captures features like implicit arguments, which do not change the
number of names bound nor their annotations, but rather how those bound names
get treated by the rest of the system. Currently, \panbench only supports
visibility-related modifiers, but is designed to be extensible;
e.g. quantities in \idris or irrelevance annotations in \agda.

The language of binding modifiers is implemented as the following set of typeclasses.
\begin{lstlisting}[style=haskell,caption={Typeclasses for binding modifiers.}]
class Implicit cell where
  implicit :: cell -> cell

class SemiImplicit cell where
  semiImplicit :: cell -> cell
\end{lstlisting}

This is a case where the granular tagless encoding shines. Both \lean and \rocq
have a form of semi-implicits\footnote{We use the term \emph{semi-implicit}
argument to refer to an implicit argument that is not eagerly instantiated. In
\rocq these are known as non-maximal implicits.}, whereas \idris and \agda do not.
Decomposing the language of binding modifiers into granular
pieces lets us write benchmarks that explicitly require support for features
like semi-implicits.  A monolithic class for the entire
language of modifiers would have forced us into runtime errors
(or, even worse, dubious attempts at translation).

\subsubsection{Top-Level Definitions}
\label{sec:top-level}

The question of top-level definitions is much thornier, and there seems to be less agreement on how
they ought to be structured. Luckily, we can re-apply many of the lessons we learned in our treatment of binders;
after all, definitions are ``just'' top-level binding forms! This perspective lets us simplify how
we view some more baroque top-level bindings. As a contrived example, consider the following signature
for a pair of top-level \agda definitions.
\begin{lstlisting}[style=agda,caption={A complicated \agda signature.}]
private instance abstract @irr @mixed foo bar : Nat -> _
\end{lstlisting}
In our language of binders, this definition consists of a \(2\)-ary annotated binding
of the names \lstinline|foo|, \lstinline|bar| that has had a sequence of binding modifiers applied to it.

Unfortunately, this insight does not offer a complete solution. Notably, our
four target grammar differ significantly in how they treat type signatures.
Those that prioritize dependent pattern matching (e.g. \agda, \idris) typically
opt to have standalone type signatures: this allow for top-level pattern
matches, which in turn makes it much easier to infer
motives~\cite{McBride-Mckinna:2004}.  Conversely, languages oriented around
tactics (e.g. \lean, \rocq) typically opt for in-line type signatures and
pattern-matching expressions. This appears to be largely independent of
the Miranda/ML syntax split: \lean borrows large parts of its syntax from Haskell,
yet still opts for in-line signatures.

This presents us with a design decision: should our idealized grammar use inline or standalone signatures?
As long as we can (easily) translate from one style to the other, we have a genuine decision to make.
We have opted for the former as standalone signatures offer variations that languages with inline
signatures cannot handle. As a concrete example, consider the following \agda declaration:

\begin{lstlisting}[style=agda,caption={A definition with mismatched names.}]
  id : (A : Type) -> A -> A
  id B x = x
\end{lstlisting}

In particular, note that we have bound the first argument to a different name. Translating this to a
corresponding \rocq declaration then forces us to choose to use either the name from the signature
or the term. Using in-line signatures does not require this unforced choice.

However, inline signatures are not completely without fault, and cause some edge cases with binding
modifiers. Consider the following two variants of the identity function.
\label{listing:id}
\begin{lstlisting}[style=agda,caption={Two variants of the identity function (\agda).},label=lst:id]
  id : {A : Type} -> A -> A
  id x = x

  id' : {A : Type} -> A -> A
  id' {A} x = x 
\end{lstlisting}

Both definitions mark the \texttt{A} argument as an implicit, but the second definition \emph{also}
binds it in the declaration. Inline type signatures lose this extra layer of
distinction. To account for this, we were forced to refine the visibility modifier system to distinguish
between ``bound'' and ``unbound'' modifiers. This extra distinction has not proved to be too onerous in
practice, and we still believe that inline signatures are the correct choice for our application.

We have encoded this decision in our idealized grammar by introducing a notion of a ``left-hand-side''
of a definition, which consists of a collection of names to be defined, and a scope to define them
under. This means that we view definitions like Listing~\ref{lst:id} not as
functions \(\mathrm{id} : (A : \mathrm{Type}) \to A \to A\) but rather as \emph{bindings}
\(A : \mathrm{Type}, x : A \vdash \mathrm{id} : A\) in non-empty contexts.
This shift in perspective has the added benefit of making the interface to
other forms of parameterised definitions entirely uniform; for instance, a
parameterised record is simply just a record with a non-empty left-hand side.

\begin{lstlisting}[style=haskell, caption={Encoding of definitions and left-hand sides.}]
class Definition lhs tm defn | defn -> lhs tm where
  (.=) :: lhs -> tm -> defn

class DataDefinition lhs ctor defn | defn -> lhs ctor where
  data_ :: lhs -> [ctor] -> defn

class RecordDefinition lhs nm fld defn | defn -> lhs nm fld where
  record_ :: lhs -> name -> [fld] -> defn
\end{lstlisting}

As an example, Listing~\ref{code:IdChain} looks as show below in \panbench. We elide
the monstrous type signature, which painfully documents all the language features used.
\begin{lstlisting}[style=haskell,caption={The IdChain test.}]
generator = GenModule "IdChain"
 [ ] \size ->
 [ [unbound $ implicit ("a" .: builtin "Type"), "x" .: "a"] 
            |- ("f" .: "a") .= "x"
 ,
 [unbound $ implicit ("a" .: builtin "Type")] 
            |- "test" .: (None .:* "a" `arr` "a") .=
            foldr (\_ tm -> app "f" [tm]) "f" [1..size]
 ]
\end{lstlisting}

\section{Conclusion}
\label{sec:conc}

This work started out as a final-year capstone project for a team of students. It
showed that a single-source benchmark was possible, along with automated reporting.
It also showed that the results of such benchmarking would be interesting. The current
authors redesigned and rewrote the prototype into the current version.

Our benchmarking revealed multiple performance problems across all four systems.
Luckily, most of our benchmarks had at least one system perform well. This suggests
that the problems we uncovered should be relatively easy to fix, as maintainers
can look to an existing implementation for inspiration.

Our infrastructure is open-source and we hope that others will be able to build on it.
We believe that our \emph{grammatical framework} is a useful abstraction, independently
of benchmarking.  We already have plans for it ourselves, for non-benchmarking applications.

We intend to extend both the \panbench benchmark suite and the supporting infrastructure.
In particular, we are interested in adding more benchmarks that stress-test conversion
and normalization performance. This would make the \panbench benchmark suite much
more useful for the development of \emph{new} proof assistants. Currently, the amount
of work required to make an experimental proof assistant production-grade is measured
in years, which is far too long. Being to test against an existing set of benchmarks that
captures known issues would let the developers of these tools avoid falling into the
same traps as their predecessors.

\bibliography{references}

\end{document}

