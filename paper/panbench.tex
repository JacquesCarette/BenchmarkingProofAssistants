\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate,anonymous]{lipics-v2021}
%This is a template for producing LIPIcs articles. 
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling autoref support, use "autoref"
%for anonymousing the authors (e.g. for double-blind review), add "anonymous"
%for enabling thm-restate support, use "thm-restate"
%for producing a PDF according the PDF/A standard, add "pdfa"

%\pdfoutput=1 %uncomment to ensure pdflatex processing (mandatatory e.g. to submit to arXiv)
%\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\usepackage{panbench}

\title{\panbench: A Comparative Benchmarking Tool for Dependently-Typed Languages}

\author{Reed Mullanix}{Department of Computing and Software, McMaster University, Canada}{mullanir@mcmaster.ca}{https://orcid.org/0000-0002-7970-4961}{}
\author{Jacques Carette}{Department of Computing and Software, McMaster University, Canada}{carette@mcmaster.ca}{https://orcid.org/0000-0001-8993-9804}{}

\authorrunning{R. Mullanix and J. Carette}

\Copyright{Reed Mullanix and Jacques Carette}


\ccsdesc[500]{Mathematics of computing~Mathematical software performance}

\keywords{Benchmarking, dependent types, testing}

\category{} %optional, e.g. invited paper

\relatedversion{} %optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversiondetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93]{Classification (e.g. Full Version, Extended Version, Previous Version}{URL to related version} %linktext and cite are optional

%\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
%\supplementdetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93, subcategory={Description, Subcategory}, swhid={Software Heritage Identifier}]{General Classification (e.g. Software, Dataset, Model, ...)}{URL to related version} %linktext, cite, and subcategory are optional

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

%\nolinenumbers %uncomment to disable line numbering

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

\begin{abstract}
We benchmark four proof assistants (\agda, \idris, \lean and \rocq) through a
single test suite. We focus our benchmarks on the basic features that all
systems based on a similar foundations (dependent type theory) have in common.
We do this by creating an ``over language'' in which to express all the
information we need to be able to output \emph{correct and idiomatic syntax}
for each of our targets. Our benchmarks further focus on ``basic engineering''
of these systems: how do they handle long identifiers, long lines, large
records, large data declarations, and so on.

Our benchmarks reveals both flaws and successes in all systems. We give a
thorough analysis of the results.

We also detail the design of our extensible system. It is designed so that
additional tests and additional system versions can easily be added. A side
effect of this work is a better understanding of the common abstract syntactic
structures of all four systems.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Production-grade implementations of dependently typed programming languages
are complicated pieces of software that feature many intricate and potentially
expensive algorithms. As such, large amounts of engineering effort has been dedicated
to optimizing these components. Unfortunately, engineering time is a finite
resource, and this necessarily means that other parts of these systems
get comparatively less attention. This often results in easy-to-miss performance problems:
we have heard anecdotes from a proof assistant developer that a na\"ive \(O(n^{2})\)
fresh name generation algorithm used for pretty-printing resulted in 100x slowdowns
in some pathological cases.

This suggests that a benchmarking suite that focuses on these simpler components
could reveal some (comparatively) easy potential performance gains. Moreover, such
a benchmarking suite would also be valuable for developers of new dependently typed
languages, as it is much easier to optimize with a performance goal in mind. This
is an instance of the classic \(m \times n\) language tooling problem: constructing
a suite of \(m\) benchmarks for \(n\) languages directly requires a quadratic
amount of work up front, and adding either a new test case or a new language to
the suite requires an additional linear amount of effort.

Like most \(m \times n\) tooling problems, the solution is to introduce
a mediating tool. In our case, we ought to write all of the benchmarks
in an intermediate language, and then translate that intermediate language to
the target languages in question. There are existing languages like Dedukti~\cite{Dedukti:2023}
or Informath~\cite{Informath:2026}
that attempt to act as an intermediary between popular proof assistants, but these
tools typically focus on translating the \emph{content} of proofs, not exact
syntactic structure. To fill this gap, we have created the \panbench system, which consists of:

\begin{enumerate}
\item An extensible embedded Haskell DSL that encodes the concrete syntax of a
  typical dependently typed language.
\item A series of compilers for that DSL to \agda~\cite{AgdaDevelopers:Agda}, \idris~\cite{Brady:2021}, \lean~\cite{Lean4:2021}, and \rocq~\cite{Rocq:2026}.
\item A benchmarking harness that can perform sandboxed builds of
  multiple versions of \agda, \idris, \lean, and \rocq.
\item An incremental build system that can produce benchmarking reports as
  static HTML files or PGF plots\footnote{All plots in this paper were produced directly by \panbench.}.
\end{enumerate}


% Reed M, 13/02/2026:
% We discussed the main contributions, and settled on
% - Concrete benchmarking stats/graphs
% - Supporting infrastructure
% - The design of panbench qua DSL

% \todo{this itemized list should be expanded into actual text}
% \begin{itemize}
% \item benchmark system engineering and scaling
% \item benchmarking of anything resembling proofs would be a major research
% project
% \item the languages (of types/expressions and of declarations) of all 4
% are quite similar in their surface expressivity, even though all possess a
% myriad of specific features that are unshared
% \end{itemize}



\section{Methodology}
\label{sec:method}

To perform reliable benchmarking, we need tooling and we need to design
a test suite. For reproducibility, we also need to document the actual
experimental setup. We will describe the tooling in detail in later sections,
for now we focus on the design of the test suite and the experimental setup.

\begin{figure}[thb]
\begin{tabular}{l|l||l|l}
\textbf{Category} & \textbf{Details} & \textbf{Category} & \textbf{Details} \\ \hline
Syntax & Newlines            & Datatypes  & Parameters \\
       & Parentheses         &            & Indices \\
Names  & Datatype            &            & Params + Indices \\
       & Data constructor    &            & Constructors \\
       & Definition          & Records & Fields \\
       & Definition lhs      &         & Parameters \\
       & Definition rhs      & Dependency & Record Fields\\
       & Lambda              &            & Datatypes Parameters\\
       & Pi                  &            & Definitions chains \\
       & Record              &            & Record chains \\
       & Record constructor  & Nesting & Id chain \\
       & Record field        &         & Id chain \(\lambda\) \\
Binders & Lambda             &         & Let \\
        & Implicits          &         & Let addition \\
Misc    & Postulates         &         & Let functions \\
Conversion & Addition        &         & \\
\end{tabular}
\caption{Our tests}
\label{tab:tests}
\end{figure}

\subsection{Designing tests}

Let us assume that we possess the following tools:
\begin{itemize}
\item a single language of tests,
\item a means to translate these tests to each of our four languages,
\item a reproducible test harness,
\item a stable installation of the four languages.
\end{itemize}
\noindent How would we design actual tests for common features?

As we said before, our aim here is to test ``basic engineering''.  For example,
how does each system handle giant files (e.g.: one million empty lines),
large names (thousands of characters long) in each syntactic category,
large numbers of fields in records, constructors in algebraic types, large
numbers of parameters or indices, long dependency chains, and so on. We
divide our tests into the following categories:
\begin{enumerate}
\item basic syntactic capabilities
\item long names in simple structures
\item large simple structures (data, record)
\item handling of nesting and dependency
\item conversion, postulates
\end{enumerate}
\noindent where we vary the ``size'' of each. Table~\ref{tab:tests} gives
more details of the available tests.

At a higher level, we asked ourselves the question: for each aspect of a
dependently typed language (lexical structure, grammatical structure,
fundamental features of dependent languages), what could be made to ``scale''?
The only features of interest remained the ones that were clearly features of
all four systems.

Note that we are \emph{not} trying to create ``intrinsically interesting'' tests,
but rather we want them to be \emph{revealing} with respect to the need for
basic infrastructure improvements.

For lack of space, we cannot show samples of all the tests, but we show
just a few that should be \emph{illustrative} of the rest\footnote{Very minor
edits made to fit the page}. The source of
all tests can be found in the accompanying material. All tests below
are from the snapshot of our ``golden'' test suite, uniformly run with a
size parameter of \(5\).

\begin{minipage}{0.49\textwidth}
\begin{lstlisting}[style=agda,caption=Nesting: Id chain (Agda),label=code:IdChain]
module IdChain where

f : {a : Set} (x : a) -> a
f x = x

test : {a : Set} -> a -> a
test = f f f f f f
\end{lstlisting}
\end{minipage}
\begin{minipage}{0.49\textwidth}
\begin{lstlisting}[style=lean,caption=Nesting: Let add (Lean),label=code:LetAdd]
def n : Nat :=
  let x0 := 1
  let x1 := x0 + x0
  let x2 := x1 + x1
  let x3 := x2 + x2
  let x4 := x3 + x3
  x4
\end{lstlisting}
\end{minipage}

\begin{lstlisting}[style=idris,caption=Binders: Lambda (Idris 2), label=code:LargeLambda]
module Main

const5 : {A : Type} -> {B0, B1, B2, B3, B4, B5 : Type} ->
  A -> B0 -> B1 -> B2 -> B3 -> B4 -> B5 -> A
const5 = \a, b0, b1, b2, b3, b4, b5 => a
\end{lstlisting}

\begin{lstlisting}[style=rocq,caption=Dependency: Record Telescope (Rocq), label=code:RecTele]
Module RecordTelescope.

Record Telescope (U : Type) (El : U -> Type) : Type := Tele
  { a0 : U
  ; a1 : forall (x0 : El a0), U
  ; a2 : forall (x0 : El a0) (x1 : El (a1 x0)), U
  ; a3 : forall (x0 : El a0) (x1 : El (a1 x0)) (x2 : El (a2 x0 x1))
    , U
  ; a4 : forall (x0 : El a0) (x1 : El (a1 x0)) (x2 : El (a2 x0 x1)) 
    (x3 : El (a3 x0 x1 x2)), U
  ; a5 : forall (x0 : El a0) (x1 : El (a1 x0)) (x2 : El (a2 x0 x1))
    (x3 : El (a3 x0 x1 x2)) (x4 : El (a4 x0 x1 x2 x3)), U
  }.

Arguments Tele {U} {El} _ _ _ _ _ _.

End RecordTelescope.
\end{lstlisting}

\subsection{Experimental setup}

All tests are run on a dedicated (older) desktop machine running on
NixOS 25.11. The CPU is CPU is a an Intel i7-2600K with 1MB L2 cache, 8MB L3
cache, running at 3.4 Ghz, equipped with 24 Gigs of DDR3 memory. This
box has no SSD drive, but this should only affect system time.

When tests are run, no other (human) activity happens on the machine.

All tests are run with a time limit of \(60\) seconds. We also tried to
use memory limits but, alas, Linux no longer reliably supports these.
Otherwise, we ran all four systems in their default configuration.

\section{Results}
\label{sec:results}

Given that our test suite has \(32\) tests, each of which produces \(3\)
different graphs, we have no room to display all \(96\) resulting
graphs\footnote{Nor can we have appendices!}. We thus choose results that
appear to be the most ``interesting''. Furthermore, other than in
Table~\ref{tab:start-up}, we will not show the \emph{System Time} as it
correlates very strongly with memory use for our particular test suite.

\begin{figure}[htb]
\begin{tabular}{l|r|r|r}
\textbf{System} & \textbf{User Time (s)} & \textbf{System Time (s)} & \textbf{Max RSS (MB)} \\ \hline
\agda  & 0.02 (0.002) & 0.01 (0.001) & 64 (0.1) \\
\idris & 0.58 (0.007) & 0.10 (0.007) & 248 (0.1) \\
\lean  & 0.14 (0.005) & 0.04 (0.005) & 307 (0.6) \\
\rocq  & 0.05 (0.004) & 0.03 (0.003) & 95 (0.05) \\
\end{tabular}
\caption{Start-up time and memory, mean with standard deviation in parentheses.}
\label{tab:start-up}
\end{figure}

Start-up time and memory use varies wildly: from a super-fast (0.02s) and
slim (64 MB) \agda to a 29 times slower \idris and 4.8 times memory consumer in \lean.
It is worth recalling that 0.1 seconds is the ``instant'' threshold.

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/Newlines/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/Newlines/rss}}
\end{minipage}
\caption{Blank lines}
\label{fig:Newline}
\end{figure}

A file with a header and a million blank lines is, of course, not intrinsically
interesting.  It is however very \emph{revealing}: we see
(Figure~\ref{fig:Newline}) that it takes \lean and \rocq no noticeable time to
deal with that, while already 100K lines causes both \agda and \idris to slow
down and consume significantly more memory (6.3Gigabytes for 10 million lines
in \idris's case).  Estimating the run-time for \agda and \idris from the slope
of the graphs, we get that both are linear.

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/LongNameDataType/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/LongNameDataType/rss}}
\end{minipage}
\caption{Long names (datatypes)}
\label{fig:LongNameDataType}
\end{figure}

What about \emph{long identifiers}? Figure~\ref{fig:LongNameDataType} shows what happens when we
use increasingly long names for data types; other ``long names'', such as for constructors,
field names of records, etc, show similar behaviour. Unlike for blank lines, all systems show
an eventual increase in time and memory use, with \agda starting earlier than others. What is most 
interesting is that \lean barely takes any more memory, even for extremely long (4 million characters)
identifiers. \agda's time here too indicates it is linear in the number of characters.

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/LargeSimpleDatatype/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/LargeSimpleDatatype/rss}}
\end{minipage}
\caption{Enumerations}
\label{fig:Enum}
\end{figure}

Figure~\ref{fig:Enum} tests simple data types (enumerations) with short constructor names.
What is remarkable here is \idris: even at \(2^{11}\) constructors, it takes no appreciable time
or memory beyond start-up.

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/LargeLambda/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/LargeLambda/rss}}
\end{minipage}
\caption{Lambda term with many variables}
\label{fig:Lambda}
\end{figure}

Figure~\ref{fig:Lambda}, a test for a single lambda term that uses its first
variable but has $n$ other (unused) variables also declared (see
Listing~\ref{code:LargeLambda}. This seems to be a ``torture'' test where
systems perform very well until they hit a wall and suddenly time out. It
appears that the underlying problem is using an enormous amount of memory.

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/RecordTelescope/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/RecordTelescope/rss}}
\end{minipage}
\caption{Record with increasingly dependent fields}
\label{fig:RecTele}
\end{figure}

Figure~\ref{fig:RecTele} is a test for ``very dependent records'', i.e. a record
where every later field depends on all previous ones, as shown in Listing~\ref{code:RecTele}.
Of course, the code size itself is quadratic in the number of such fields.  As expected,
this rapidly takes quite a lot of time; less expected is the memory usage also goes up
very significantly. This particular test likely needs finer sampling (and maybe larger
timeouts) to understand the behaviour of each system.

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/IdChain/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/IdChain/rss}}
\end{minipage}
\caption{Chain of calls to identity function}
\label{fig:IdChain}
\end{figure}

Figure~\ref{fig:IdChain}, corresponding to Listing~\ref{code:IdChain}, nested calls to
an identity function, shows even more
extreme behaviour: basically instantaneous until memory explosion and time out. This is
well-known to be a problem for type systems based on Hindley-Milner inference, but it is
less clear that it ought to be a weakness for bidirectional typing as well. Closer
sampling (not shown) does not change this picture.

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/NestedLetAdditions/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/NestedLetAdditions/rss}}
\end{minipage}
\caption{Nested let bindings, simple addition on rhs}
\label{fig:NestedLetAdd}
\end{figure}

Figure~\ref{fig:NestedLetAdd}, corresponding to Listing~\ref{code:LetAdd}, nested lets
doing very simple arithmetic. The contrast is remarkable: \agda and \idris time out already
at size \(2^5\), \lean takes an increasing amount of time, while \rocq takes no time nor any
memory! Note the enormous amount of memory taken by \agda when it times out.

\begin{minipage}{0.49\textwidth}
\begin{lstlisting}[style=idris,caption=Conversion: Addition (Idris),label=code:ConvAdd]
conv : 5 + 5 + 5 + 5 + 5 + 0 = 25
conv = Refl
\end{lstlisting}
\end{minipage}

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/ConversionAddition/user}}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\scalebox{0.75}{\input{pgfs/ConversionAddition/rss}}
\end{minipage}
\caption{Conversion for Natural Number Addition}
\label{fig:ConvAdd}
\end{figure}

Figure~\ref{fig:ConvAdd}, corresponding to Listing~\ref{code:ConvAdd}, does simple
natural number arithmetic and ensures the correct result is obtained. Here the roles
are inverted: \agda, \idris, and \lean take no time while \rocq takes an increasing amount
of time until timeout.


% Order:
% - very basic syntactic
% - long names of simple structures
% - nested structures
% - dependency
% - conversion
%
% Meta:
% - "get worse" is when the curve is not a straight line; these are log-log plots,
%     so that indicates exponential behaviour
% - system time variance hard to explain, though seems largely related to memory
%
% --------------
% Empty
% Meta: x-axis should not be log but uniform
% Comment: this is basically measuring start-up time, which varies a lot
%  - agda is very fast, lean is a bit slow, idris2 quite slow
%  - on max RSS, lean and idris2 are very greedy
% 
% Newlines
%
% - rocq and lean survive and only get measurably worse at n = 10^7
% - idris2 times out at the end
% - agda and idris2 get worse already at 10,000
% - memory goes up for all, but *much* worse for Agda/Idris2
% 
% Parens
%
% - there must a bug in lean's handling of parents: it is fine until 256 then dies
% - idris dies at 2^16, rocq at 2^14, while agda survives 2^16
% - (should compute slope of that straight line in idris2 and agda memory usage)
% 
% --------------
% 
% Meta: all LongName are fairly similar (good!) but still not entirely
%
% LongNameDatatype
%
% - idris2 handles quite smoothly until giant length
% - agda blows up much earlier
% - unknown why super-short length is (significantly) worse
% - lean's memory use is great, rest blow up, agda worst
% 
% LongNameDatatypeConstructor
% 
% - fairly similar (to above) except idris2 much more stable and rocq's memory use stabilizes
%
% LongNameDefinition
% 
% - similar to above but irdris2 blows up (didn't above(
%
% LongNameDefinitionLhs
%
% - idris2 now does not really blow up (though it starts to consume memory)
% - memory
% 
% LongNameDefinitionRhs
% 
% - same as above
%
% LongNameLambda
% 
% - long name lambda and Pi are 'the same', which is good (except for the blow up
%  of Agda and Idris2)
%
% LongNamePi
% 
% - see above
%
% - next 3 again follow same pattern
% LongNameRecord
% 
% LongNameRecordConstructor
% 
% LongNameRecordField
% 
% --------------
%
% DatatypeParameters
%
% - impressive performance (minus start-up) from idris2 and rocq
% - agda and lean "blow up" around size 30
% 
% LargeIndexedDatatype
% 
% - all systems get worse, and all but rocq time out at n=256
% - Idris goes from its usual slow to timing out entirely with no transition
%
% LargeIndexedParameterisedDatatype
% 
% - idris2 rocks this one, with agda a close second
% - rocq explodes wrt memory, and lean on time
%
% LargeLambda
%
% - idris2 plateau unexplained; decrease in memory even more so
% - lean's spike?
% - agda suddenly blows up (seems to be memory related)
% 
% LargeSimpleDatatype
% 
% - idris2 does extremely well here
% - all others (slowly) get worse 
%
% LargeSimpleRecord
% 
% - idris2 does very well until the very end and blows up
% - agda and rocq handle very well
% - lean takes an increasingly large amount of time while rocq's memory blows up
% 
% ManyImplicits
% 
% - extremely weird idris2 behaviour: flat then spike
% - rocq and agda time curve up very gently, lean more seriously
% - rocq and agda start to consume more memory (from a very low level)
%
% * go up one more notch?
%
% Postulates
%
% - idris2 gets worse quickly, times out at 2^15
% - same for lean, times out earlier at 2^13
% - rocq and agda survive but seems to be in O(n^2) ? territory?
% 
% RecordParameters
% 
% - only rocq survives, rest die at 2^9
% - not clear what causes the problem
%
% SequentialDefinitions
% 
% - rocq clearly has worse definition (different slope), but memory behaviour is odd
% 
% SequentialSimpleRecords
%
% - idris2 seems to handle this extremely well
% - rest eventually starts to goblle memory
% - lean really does not handle this well at all
% 
% SimpleDataDefinitions
%
% - lean times out
% - same slope for rocq/agda and likely lean; not clear for idris2
% ----------------
%
% LargeDependentRecord
%
% all systems get worse, following a similar path, 
% rest time out (lean at n=128, rest at n=256), only rocq survives
% memory use goes way way up
% 
% SequentialDependentRecords
%
% - lean times out quickly, Idris eventually
% - none seem to "level out" to straight lines
% 
% ----------------
% 
% NestedLet
% 
% - idris2 times out(n=1024), as does lean (n=512), rest do fine
%
% * another notch?
%
% NestedLetAdditions
% 
% - lots of tiem outs (agda, idris2 already after n=16), lean gets worse but survives,
%   rocq kills it
%
% NestedLetFunctions
% 
% - all systems blow up, at different points, as well as consume a lot of memory
% - lean, idris2 time out at n=128, agda at n=256, rocq survives
%
% IdChain
%
% - all systems blow up and time out (seems driven by memory use) between 8 and 16
%    except for lean, which uses no more memory at 32 and just a touch more time
% 
% ConversionAddition
% - Agda's higher time for the small case is unexplained
% - rocq is the only one that "blows up" with size
%

\section{Discussion}
\label{sec:discuss}

The previous section analyzed the results themselves. Here we speculate on why the results may be as they
are.

\subsection{\agda}

Most the results for \agda follow a general pattern: it is consistently the
fastest and most memory efficient for small inputs, but has a couple of edge
cases where it really struggles. Notably, \agda performs rather poorly on tests
that put heavy pressure on the parser. This poor performance can be explained
by the use of Haskell's \lstinline|String| inside of Agda's lexer, which is a
known performance pitfall.

\agda also struggles to typecheck let-bindings that introduce some non-trivial
sharing. This is an unfortunate consequence of \agda's choice to inline let
bindings during elaboration, which can easily result in exponential blowups
if sharing is lost.

\subsection{\idris}

Unfortunately, \idris struggles on a lot of these tests. It has the highest startup time
of any systems we benchmarked, coming in at over half a second. We suspect that this can
be explained by the choice to use scheme as a runtime, which incurs a high interpreter
startup cost. However, we have not confirmed this.

Like \agda, \idris struggles on benchmarks that stress-test the parser. Notably, it
is the only system that times out when trying to parse files containing only  \(10^{7}\) newlines,
and consumes over 6 gigabytes while doing so. We suspect that this can be attributed to
\idris's \texttt{Text.Lexer} library, though we have not thoroughly investigated this.
It also struggles with let-bindings, and times out when trying to elaborate a linear sequence of 1024
let bindings. This does not appear to have the same root cause as \agda's poor performance,
and further investigation is required.

However, there are some bright spots. \idris performs \emph{exceptionally} well on all tests
involving elaboration of datatypes and records once the startup time is accounted for.

\subsection{\lean}

Initially, we expected that \lean would be one of the best performers. Thus,
we were then rather surprised by the number of time outs and hard crashes
we encountered. Many tests involving records and datatypes hit hard limits
for the number of fields, indices, or constructors. It also seems to struggle
on elaboration tasks that involve large numbers of names: we suspect that
this could be due to the use of a locally-nameless
representation~\cite{Chargueraud:2012} internally, but we were not able to confirm our suspicion. 

On the bright side, \lean does manage to elaborate the nested addition test,
though it does seem to exhibit some exponential behaviour while doing so.
It also is able to easily handle almost all of the parsing tests with ease,
though it does stack overflow after 512 nested parenthesis. It was also the
only system that was able to handle checking 32 iterations of \(\mathrm{id} \mathrm{id} \cdots \mathrm{id}\),
and did so with ease, taking only \(\approx 0.25\) seconds and a negligible
amount of additional memory.

\subsection{\rocq}

Out of all of the systems we measured, \rocq was by far and away the winner.
On most tests, it typically came in first or second place. Notably, it
was the only system that was able to handle the nested addition test without
exhibiting some sort of exponential blow-up. It also consistently outperforms
other systems on parsing-heavy tasks, though it does stack overflow when
presented with 16384 nested parenthesis.

However, there were still some surprises. \rocq was the only system that
exhibited linear runtime on the addition conversion test: all other systems
managed to run in essentially constant time. It also struggled to handle
files that contained large numbers of very simple definitions, and took nearly
45 seconds to typecheck 4096 of them. We suspect that both of these may have
to do with a sub-optimal approach to checking natural numbers, but were not
able to confirm.

\section{Infrastructure}
\label{sec:infrastructure}

One of the major goals of \panbench is to make performance analysis as
low-cost as possible for language developers. Meeting this goal requires
a large amount of supporting infrastructure: simply generating benchmarks
is not very useful if you cannot run them nor analyze their outcomes. After some
discussion, we concluded that any successful language benchmarking system should
meet the following criteria:

\begin{enumerate}
\item It must provide infrastructure for performing sandboxed builds of
  compilers from source. Asking potential users to set up four different
  toolchains presents an extremely large barrier to adoption. Moreover,
  if we rely on user-provided binaries, then we have no hope of
  obtaining reproducible results, which in turn makes any insights
  far less actionable.
\item It must allow for multiple revisions of the same tool to be installed
  simultaneously. This enables developers to easily look for performance regressions,
  and quantify the impact of optimizations.
\item It must allow for multiple copies of the \emph{same} version tool to be installed
  with different build configurations. This allows developers to look for performance regressions
  induced by different compiler versions/optimizations.
\item It must be able to be run locally on a developers machine. Cloud-based
  tools are often cumbersome to use and debug, which in turn lowers adoption.
\item It must present a declarative interface for creating benchmarking environments and running benchmarks.
  Sandboxed builds of tools are somewhat moot if we cannot trust that a benchmark was
  run with the correct configuration.
\item It must present performance results in a self-contained format that is easy to understand
  and share. Performance statistics that require large amounts of post-processing
  or dedicated tools to view can not be easily shared with developers, which in
  turn makes the data less actionable.
\end{enumerate}

Of these criteria, the first four present the largest engineering challenge, and are
tantamount to constructing a meta-build system that is able orchestrate \emph{other} build
systems. We approached the problem by constructing a bespoke content-addressed
system atop of Shake~\cite{Mitchell:2012}, which we discuss in section \ref{sec:builds}.
The final two criteria also presented some unforseen difficulties, which we detail in \ref{sec:reports}.

\subsection{The \panbench Build System}
\label{sec:builds}

As noted earlier, we strongly believe that any benchmarking system should provide infrastructure
for installing multiple versions of reproducibly built software. Initially, we intended to build
this infrastructure for \panbench atop of Nix~\cite{Dolstra:Nix}. This is seemingly a perfect fit;
after all, Nix was designed to facilitate almost exactly this use-case. However, after further deliberation,
we came to the conclusion that Nix did not quite meet our needs for the following reasons: 

\begin{enumerate}
\item Nix does not work natively on Windows. Performance problems
  can be operating system specific, so ruling out an OS that has
  a large user base that is often overlooked in testing seems
  unwise\footnote{Currently, \panbench does not support Windows, but this
  is an artifact of prioritization, and not a fundamental restriction.}.
\item Nix adds a barrier to adoption. Installing Nix is a somewhat
  invasive process, especially on MacOS\footnote{The situation is even worse on x86-64 Macs, which most Nix installers
  simply do not support.}.
We believe that it is somewhat unreasonable to ask developers to add users and modify
their root directory to run a benchmarking tool, and strongly suspect that this would
hamper adoption.
\end{enumerate}

With the obvious option exhausted, we opted to create our own Nix-inspired 
build system based atop Shake~\cite{Mitchell:2012}. This avoids the aforementioned
problems with Nix: Shake works on Windows, and only requires potential users
to install a Haskell toolchain.

The details of content-addressed build systems are a deep topic unto themselves,
so we will only describe the key points. Systems like Nix use an
\emph{input-addressing} scheme, wherein the results of a build are
stored on disk prefixed by a hash of all build inputs. Crucially, this lets the build
system know where to store the result of the build before the build is run, which
avoids vicious cycles where the result of a build depends on its own hash. However,
most input-addressed systems also require that the hash of the inputs \emph{solely}
determines the output. On its face, this is a reasonable ask, but taking this
idea to its logical conclusion requires one to remove \emph{any} dependency
on the outer environment, which in turn forces one to re-package the entirety of the software
stack all the way down to \texttt{libc}. This is an admirable goal in its own right,
but is actually somewhat counterproductive for our use case: there is a very real chance
that we might end up benchmarking our sub-par repackaging of some obscure C dependency
four layers down the stack.

To avoid this cascading series of dependency issues, \panbench takes a hybrid approach,
wherein builds are input-addressed, but are allowed to also depend on the external
environment. Additionally, the results of builds are also hashed, and stored out-of-band
inside of a Shake database. This hash is used to invalidate downstream
results, and also act as a fingerprint to identify if two benchmarks were created from the
same binary. This enables a pay-as-you go approach to reproducibility, which we hope
will result in a lower adoption cost. Moreover, we can achieve fully reproducible builds
by using Nix as a meta-meta build system to compile \panbench: this is how we obtained
the results presented in Section \ref{sec:results}.

\subsection{Running Benchmarks and Generating Reports}
\label{sec:reports}

As noted in the introduction to this section, we believe that benchmarking
tools should present a \emph{declarative} interface for writing not just single
benchmarking cases, but entire benchmarking suites and their corresponding environments.
\panbench accomplishes this by \emph{also} implementing the benchmark execution framework
atop Shake. This lets us easily integrate the process of tool installation with environment
setup, but introduces its own set of engineering challenges.

The crux of the problem is that performance tests are extremely sensitive to the current
load on the system. This is largely at odds with the goals of a build system, which
is to completely saturate all system resources to try to complete a build as fast
as possible. This can be avoided via careful use of locks, but we are then faced with
another, larger problem. Haskell is a garbage collected language, and running the GC
can put a pretty heavy load on the system. Moreover, the GHC runtime system is very
well engineered, and is free to run the garbage collector inside of \texttt{safe} FFI
calls, and waiting for process completion is marked as \texttt{safe}.

To work around this, we opt to eschew existing process interaction
libraries, and implement the benchmark spawning code in C\footnote{This is why \panbench does not currently support Windows.}.
This lets us take the rather extreme step of linking against the GHC runtime system
so that we can call \texttt{rts_pause}, which pauses all other Haskell threads and GC
sweeps until \texttt{rts_resume} is called.

Initially, we thought that this was the only concern that would arise
by tightly integrating the build system with the benchmark executor.
However, our initial benchmarks on Linux systems displayed some very strange
behaviour, wherein the max resident set size reported by \texttt{getrusage}
and \texttt{wait4} would consistently report a reading of approximately
2 gigabytes halfway through a full benchmarking suite. After some investigating,
we discovered the Linux preserves resource usage statistics across calls to \texttt{execve}.
Consequentially, this means that we are unable to measure any max RSS that
is lower than max RSS usage of \panbench itself. Luckily, our lowest baseline is Agda at 64 megs,
and we managed to get the memory usage of \panbench itself down to ~10 megs via
some careful optimization and GC tuning.

Currently, the statistics that \panbench gathers can then be rendered into standalone HTML
files with \texttt{vega-lite} plots, or into \TeX files containing PGF plots. We intend to add more
visualization and statistic analysis tools as the need arises.

\section{The Design of \panbench}
\label{sec:design}

At its core, \panbench is a tool for producing grammatically well-formed concrete
syntax across multiple different languages. Crucially, \panbench does \emph{not}
require that the syntax produced is well-typed or even well-scoped: if it did,
then it would be impossible to benchmark how systems perform when they encounter
errors. This seemingly places \panbench in stark contrast with other software tools\todo{Reed: Cite something} for
working with the meta-theoretic properties of type systems, which are typically
concerned only with well-typed terms.

However, the core task of \panbench is not that different from the task of
a logical framework~\cite{Harper-Honsell-Plotkin:1993}: Both
exist to manipulate judgements, inference rules, and derivations:
\panbench just works with \emph{grammatical} judgements and production
rules rather than typing judgments and inference rules. In this sense
\panbench is a \emph{grammatical} framework\footnote{Not to be
  confused with \emph{the} Grammatical Framework~\cite{Ranta:2011},
  which aims to be a more natural-language focused meta-framework for
  implementing and translating between grammars.  } rather than a
logical one.

This similarity let us build \panbench atop well-understood design 
principles. In particular, a mechanized logical framework typically
consists of two layers:

\begin{enumerate}
\item A layer for defining judgements \`{a} la relations.
\item A logic programming layer for synthesizing derivations.
\end{enumerate}

To use a logical framework, one first encodes a language by laying out all of the
judgements. Then, one needs to prove an adequacy theorem on the side that
shows that their encoding of the judgements actually aligns with the language. However, if
one wanted to mechanize this adequacy proof, then a staged third layer that consists of a more
traditional proof assistant would be required.

If we take this skeleton design and transpose it to work with grammatical constructs
rather than logical ones, we will also obtain three layers:

\begin{enumerate}
\item A layer for defining grammars as relations.
\item A logic programming layer for synthesizing derivations.
\item A staged functional programming layer for proving ``adequacy'' results.
\end{enumerate}

In this case, an adequacy result for a given language \(\mathcal{L}\) is a constructive proof that
all grammatical derivations written within the framework can be expressed within the
concrete syntax of a language \(\mathcal{L}\). However, the computational content of such a proof
essentially amounts to a compiler written in the functional programming layer. Given that this
compiler outputs \emph{concrete syntax}, it is implemented as a pretty-printer.

\subsection{Implementing The Grammatical Framework}
\label{sec:framework-implementation}

Implementing a bespoke hybrid of a logic and functional programming language is
no small feat, and also requires prospective users to learn yet another single-purpose
tool. Luckily, there already exists a popular, industrial-grade hybrid logic/functional
programming language in wide use: GHC Haskell.

At first glance, Haskell does not contain a logic programming language. However,
if we enable enough GHC extensions, the typeclass system can be made \emph{just}
rich enough to encode a simply-typed logical framework. The key insight is that
we can encode each production rule using multi-parameter type classes with
a single method. Moreover, we can encode our constructive adequacy proofs for a
given set of production rules as instances that translate each of the productions
in the abstract grammar to productions in the syntax of an actual language.

As a concrete example, consider the grammar of the following simple imperative language.
\begin{grammar}
  <expr> := x | n | <expr> `+' <expr> | <expr> `*' <expr>
  
  <stmt> := <var> `=' <expr> | `while' <expr> `do' <stmt> | <stmt> `;' <stmt>
\end{grammar}

We can then encode this grammar with the following set of multi-parameter typeclasses:

\begin{lstlisting}[style=haskell,caption={An example tagless encoding.}]
class Var expr where
  var :: String -> expr

class Lit expr where
  lit :: Int -> expr

class Add expr where
  add :: expr -> expr -> expr

class Mul expr where
  mul :: expr -> expr -> expr

class Assign expr stmt where
  assign :: String -> expr -> stmt

class While expr stmt where
  while :: expr -> stmt -> stmt

class AndThen stmt where
  andThen :: stmt -> stmt -> stmt
\end{lstlisting}

This style of language encoding is typically known as the untyped variant of \emph{finally tagless}~\cite{Carette-Kiselyov-Shan:2009}.
However, our encoding is a slight refinement where
we restrict ourselves to a single class per production rule. Other
tagless encodings often use a class per syntactic category. This more fine-grained approach
allows us to encode grammatical constructs that are only supported by a subset of our target
grammars; see section \ref{sec:language-design} for examples.

Unfortunately, the encoding above has some serious ergonomic issues. In particular, expressions
like \lstinline|assign "x" (lit 4)| will result in an unsolved metavariable for \lstinline|expr|,
as there may be multiple choices of \lstinline|expr| to use when solving the \lstinline|Assign ?expr stmt|
constraint. Luckily, we can resolve ambiguities of this form through judicious use of functional dependencies~\cite{Jones:2000},
as demonstrated below.

\begin{lstlisting}[style=haskell, caption={A tagless encoding with functional dependencies.}]
class Assign expr stmt | stmt -> expr where
  assign :: String -> expr -> stmt

class While expr stmt | stmt -> expr where
  while :: expr -> stmt -> stmt
\end{lstlisting}

\subsection{Designing The Language}
\label{sec:language-design}

Now that we've fleshed out how we are going to encode our grammatical framework into our host language,
it's time to design our idealized abstract grammar. All of our target languages roughly agree on a subset of
the grammar of non-binding terms: the main sources of divergence are binding
forms and top-level definitions\footnote{As we shall see in section
\ref{sec:top-level}, the syntactical divergence of top-level definitions is
essentially about binders as well.}.
This is ultimately unsurprising: dependent type theories are fundamentally theories of binding and
substitution, so we would expect some variation in how our target languages present the core of
their underlying theories.

This presents an interesting language design problem. Our idealized grammar will need
to find ``syntactic abstractions'' common between our four target languages.  Additionally,
we would also like for our solution to be (reasonably) extensible. Finding the core
set of grammatical primitives to accomplish this task is surprisingly tricky, and requires
a close analysis of the fine structure of binding.

\todo{JC: maybe a linguistic analogy would be useful? SVO vs SOV requires us to notice the
syntactic categories subject, object verb that are common to all, and that explicit renderings
merely differ in the order. Similarly for singular and plural as modifiers.}

\subsubsection{Binding Forms}
\label{sec:binding-form}

As users of dependently typed languages are well aware, a binding form carries much more information than
just a variable name and a type. Moreover, this extra information can have a large impact on typechecking
performance, as is the case with implicit/visible arguments. To further complicate matters, languages often offer
multiple syntactic options for writing the same binding form, as is evidenced by the prevalence of multi-binders
like \((x\ y\ z : A) \to B\). Though such binding forms are often equivalent to their single-binder counterparts
as \emph{abstract} syntax, they may have different performance characteristics, so we cannot simply lower them to a
uniform single-binding representation. To account for these variations, we have designed a sub-language dedicated solely
to binding forms. This language classifies the various binding features along three separate axes: binding arity,
binding annotations, and binding modifiers.

Binding arities and annotations are relatively self-explanatory, and classify the number of names bound, along with
the type of annotation allowed. Our target languages all have their binding arities falling into one of three classes:
\(n\)-ary, unary, or nullary. We can similarly characterise annotations into three categories: required, optional, or forbidden.

This language of binders is encoded in the implementation as a single class \lstinline|Binder| that is parameterised
by higher-kinded types \lstinline|arity :: Type -> Type| and \lstinline|ann :: Type -> Type|, and provide standardized
types for all three binding arities and annotations.  

\begin{lstlisting}[style=haskell,caption={The basic binding constructs in \panbench.}]
class Binder arity nm ann tm cell | cell -> nm tm where
  binder :: arity nm -> ann tm -> cell

-- | No annotation or arity.
data None nm = None
  
-- | A single annotation or singular arity.
newtype Single a = Single { unSingle :: a }

-- | Multi-binders.
type Multi = []

-- | Infix operator for an annotated binder with a single name.
(.:) :: (Binder Single nm Single tm cell) => nm -> tm -> cell
nm .: tp = binder (Single nm) (Single tp)

-- | Infix operator for an annotated binder.
(.:*) :: (Binder arity nm Single tm cell) => arity nm -> tm -> cell
nms .:* tp = binder nms (Single tp)
\end{lstlisting}
Production rules that involve binding forms are encoded as classes that are parametric
over a notion of a binding cell, as demonstrated below.
\begin{lstlisting}[style=haskell, caption={The \panbench class for \(\Pi\)-types.}]
class Pi cell tm | tm -> cell where
  pi :: [cell] -> tm -> tm
\end{lstlisting}

Decoupling the grammar of binding forms from the grammar of binders themselves allows us
to be somewhat polymorphic over the language of binding forms when writing generators.
This in turn means that we can potentially re-use generators when extending \panbench with
new target grammars that may support only a subset of the binding features present in
our four target grammars.

Binding modifiers, on the other hand, require a bit more explanation. A binding modifier captures features like
implicit arguments, which do not change the number of names bound nor their annotations, but rather how
those bound names get treated by the rest of the system. Currently,
\panbench only supports visibility-related modifiers, but we have designed the system so that it is
easy to extend with new modifiers; e.g. quantities in \idris or irrelevance annotations in \agda.

The language of binding modifiers is implemented as the following set of Haskell typeclasses.
\begin{lstlisting}[style=haskell,caption={Typeclasses for binding modifiers.}]
class Implicit cell where
  implicit :: cell -> cell

class SemiImplicit cell where
  semiImplicit :: cell -> cell
\end{lstlisting}

This is a case where the granular tagless encoding shines. Both \lean and \rocq have a form of semi-implicits\footnote{We use the term
  \emph{semi-implicit} argument to refer to an implicit argument that is not eagerly instantiated. In \rocq these are known as
  non-maximal implicits.}, whereas \idris and \agda have no such notion. Decomposing the language of binding
modifiers into granular pieces lets us write benchmarks that explicitly require support for features like semi-implicits.
Had we used a monolithic class that encodes the entire language of modifiers, we would have to resort to runtime errors
(or, even worse, dubious attempts at translation).

\subsubsection{Top-Level Definitions}
\label{sec:top-level}

The question of top-level definitions is much thornier, and there seems to be less agreement on how
they ought to be structured. Luckily, we can re-apply many of the lessons we learned in our treatment of binders;
after all, definitions are ``just'' top-level binding forms! This perspective lets us simplify how
we view some more baroque top-level bindings. As a contrived example, consider the following signature
for a pair of top-level \agda definitions.
\begin{lstlisting}[style=agda,caption={A complicated \agda signature.}]
private instance abstract @irr @mixed foo bar : Nat -> _
\end{lstlisting}
In our language of binders, this definition consists of a \(2\)-ary annotated binding
of the names \lstinline|foo|, \lstinline|bar| that has had a sequence of binding modifiers applied to it.

Unfortunately, this insight does not offer a complete solution. Notably, our four
target grammar differ significantly in how their treatment of type signatures.
prioritize dependent pattern matching (e.g. \agda, \idris) typically opt to have standalone type signatures:
this allow for top-level pattern matches, which in turn makes it much easier to infer motives\cite{McBride-Mckinna:2004}.
Conversely, languages oriented around tactics (e.g. \lean, \rocq) typically opt for in-line type signatures and pattern-matching
expressions. This appears to be largely independent of Miranda/ML syntax split: \lean borrows large parts of
its syntax from Haskell, yet still opts for in-line signatures.

This presents us with a design decision: should our idealized grammar use inline or standalone signatures?
As long as we can (easily) translate from one style to the other, we have a genuine decision to make.
We have opted for the former as standalone signatures offer variations that languages with inline
signatures cannot handle. As a concrete example, consider the following \agda declaration:

\begin{lstlisting}[style=agda,caption={A definition with mismatched names.}]
  id : (A : Type) -> A -> A
  id B x = x
\end{lstlisting}

In particular, note that we have bound the first argument to a different name. Translating this to a
corresponding \rocq declaration then forces us to choose to use either the name from the signature
or the term. Conversely, using in-line signatures does not lead us to having to make an unforced
choice when translating to a separate signature, as we can simply duplicate the name in both the
signature and term.

However, inline signatures are not completely without fault, and cause some edge cases with binding
modifiers. As an example, consider the following two variants of the identity function in \agda.
\label{listing:id}
\begin{lstlisting}[style=agda,caption={Two variants of the identity function.},label=lst:id]
  id : {A : Type} -> A -> A
  id x = x

  id' : {A : Type} -> A -> A
  id' {A} x = x 
\end{lstlisting}

Both definitions mark the \texttt{A} argument as an implicit, but the second definition \emph{also}
binds it in the declaration. When we pass to inline type signatures, we lose this extra layer of
distinction. To account for this, we were forced to refine the visibility modifier system to distinguish
between ``bound'' and ``unbound'' modifiers. This extra distinction has not proved to be too onerous in
practice, and we still believe that inline signatures are the correct choice for our application.

We have encoded this decision in our idealized grammar by introducing a notion of a ``left-hand-side''
of a definition, which consists of a collection of names to be defined, and a scope to define them
under. This means that we view definitions like Listing~\ref{lst:id} not as
functions \(\mathrm{id} : (A : \mathrm{Type}) \to A \to A\) but rather as \emph{bindings}
\(A : \mathrm{Type}, x : A \vdash \mathrm{id} : A\) in non-empty contexts.
This shift in perspective has the added benefit of making the interface to
other forms of parameterised definitions entirely uniform; for instance, a
parameterised record is simply just a record with a non-empty left-hand side.

In \panbench, definitions and their corresponding left-hand sides are encoded
via the following set of typeclasses.
\begin{lstlisting}[style=haskell, caption={Definitions and left-hand sides.}]
class Definition lhs tm defn | defn -> lhs tm where
  (.=) :: lhs -> tm -> defn

class DataDefinition lhs ctor defn | defn -> lhs ctor where
  data_ :: lhs -> [ctor] -> defn

class RecordDefinition lhs nm fld defn | defn -> lhs nm fld where
  record_ :: lhs -> name -> [fld] -> defn
\end{lstlisting}

\todo{JC: should have a closing sentence}
\section{Conclusion}
\label{sec:conc}

\bibliography{references}

% \section{Charts}

% \subsection{Empty}

% \input{pgfs/Empty/user.tex}
% \\
% \input{pgfs/Empty/system.tex}
% \\
% \input{pgfs/Empty/rss.tex}

% \subsection{ConversionAddition}
% \input{pgfs/ConversionAddition/user.tex}
% \\
% \input{pgfs/ConversionAddition/system.tex}
% \\
% \input{pgfs/ConversionAddition/rss.tex}

% \subsection{DatatypeParameters}
% \input{pgfs/DatatypeParameters/user.tex}
% \\
% \input{pgfs/DatatypeParameters/system.tex}
% \\
% \input{pgfs/DatatypeParameters/rss.tex}

% \subsection{IdChain}
% \input{pgfs/IdChain/user.tex}
% \\
% \input{pgfs/IdChain/system.tex}
% \\
% \input{pgfs/IdChain/rss.tex}

% \subsection{LargeDependentRecord}
% \input{pgfs/LargeDependentRecord/user.tex}
% \\
% \input{pgfs/LargeDependentRecord/system.tex}
% \\
% \input{pgfs/LargeDependentRecord/rss.tex}

% \subsection{LargeIndexedDatatype}
% \input{pgfs/LargeIndexedDatatype/user.tex}
% \\
% \input{pgfs/LargeIndexedDatatype/system.tex}
% \\
% \input{pgfs/LargeIndexedDatatype/rss.tex}

% \subsection{LargeIndexedParameterisedDatatype}
% \input{pgfs/LargeIndexedParameterisedDatatype/user.tex}
% \\
% \input{pgfs/LargeIndexedParameterisedDatatype/system.tex}
% \\
% \input{pgfs/LargeIndexedParameterisedDatatype/rss.tex}

% \subsection{LargeLambda}
% \input{pgfs/LargeLambda/user.tex}
% \\
% \input{pgfs/LargeLambda/system.tex}
% \\
% \input{pgfs/LargeLambda/rss.tex}

% \subsection{LargeSimpleDatatype}
% \input{pgfs/LargeSimpleDatatype/user.tex}
% \\
% \input{pgfs/LargeSimpleDatatype/system.tex}
% \\
% \input{pgfs/LargeSimpleDatatype/rss.tex}

% \subsection{LargeSimpleRecord}
% \input{pgfs/LargeSimpleRecord/user.tex}
% \\
% \input{pgfs/LargeSimpleRecord/system.tex}
% \\
% \input{pgfs/LargeSimpleRecord/rss.tex}

% \subsection{LongNameDatatype}
% \input{pgfs/LongNameDatatype/user.tex}
% \\
% \input{pgfs/LongNameDatatype/system.tex}
% \\
% \input{pgfs/LongNameDatatype/rss.tex}

% \subsection{LongNameDatatypeConstructor}
% \input{pgfs/LongNameDatatypeConstructor/user.tex}
% \\
% \input{pgfs/LongNameDatatypeConstructor/system.tex}
% \\
% \input{pgfs/LongNameDatatypeConstructor/rss.tex}

% \subsection{LongNameDefinition}
% \input{pgfs/LongNameDefinition/user.tex}
% \\
% \input{pgfs/LongNameDefinition/system.tex}
% \\
% \input{pgfs/LongNameDefinition/rss.tex}

% \subsection{LongNameDefinitionLhs}
% \input{pgfs/LongNameDefinitionLhs/user.tex}
% \\
% \input{pgfs/LongNameDefinitionLhs/system.tex}
% \\
% \input{pgfs/LongNameDefinitionLhs/rss.tex}

% \subsection{LongNameDefinitionRhs}
% \input{pgfs/LongNameDefinitionRhs/user.tex}
% \\
% \input{pgfs/LongNameDefinitionRhs/system.tex}
% \\
% \input{pgfs/LongNameDefinitionRhs/rss.tex}

% \subsection{LongNameLambda}
% \input{pgfs/LongNameLambda/user.tex}
% \\
% \input{pgfs/LongNameLambda/system.tex}
% \\
% \input{pgfs/LongNameLambda/rss.tex}

% \subsection{LongNamePi}
% \input{pgfs/LongNamePi/user.tex}
% \\
% \input{pgfs/LongNamePi/system.tex}
% \\
% \input{pgfs/LongNamePi/rss.tex}

% \subsection{LongNameRecord}
% \input{pgfs/LongNameRecord/user.tex}
% \\
% \input{pgfs/LongNameRecord/system.tex}
% \\
% \input{pgfs/LongNameRecord/rss.tex}

% \subsection{LongNameRecordConstructor}
% \input{pgfs/LongNameRecordConstructor/user.tex}
% \\
% \input{pgfs/LongNameRecordConstructor/system.tex}
% \\
% \input{pgfs/LongNameRecordConstructor/rss.tex}

% \subsection{LongNameRecordField}
% \input{pgfs/LongNameRecordField/user.tex}
% \\
% \input{pgfs/LongNameRecordField/system.tex}
% \\
% \input{pgfs/LongNameRecordField/rss.tex}

% \subsection{ManyImplicits}
% \input{pgfs/ManyImplicits/user.tex}
% \\
% \input{pgfs/ManyImplicits/system.tex}
% \\
% \input{pgfs/ManyImplicits/rss.tex}

% \subsection{NestedLet}
% \input{pgfs/NestedLet/user.tex}
% \\
% \input{pgfs/NestedLet/system.tex}
% \\
% \input{pgfs/NestedLet/rss.tex}

% \subsection{NestedLetAdditions}
% \input{pgfs/NestedLetAdditions/user.tex}
% \\
% \input{pgfs/NestedLetAdditions/system.tex}
% \\
% \input{pgfs/NestedLetAdditions/rss.tex}

% \subsection{NestedLetFunctions}
% \input{pgfs/NestedLetFunctions/user.tex}
% \\
% \input{pgfs/NestedLetFunctions/system.tex}
% \\
% \input{pgfs/NestedLetFunctions/rss.tex}

% \subsection{Newlines}
% \input{pgfs/Newlines/user.tex}
% \\
% \input{pgfs/Newlines/system.tex}
% \\
% \input{pgfs/Newlines/rss.tex}

% \subsection{Parens}
% \input{pgfs/Parens/user.tex}
% \\
% \input{pgfs/Parens/system.tex}
% \\
% \input{pgfs/Parens/rss.tex}

% \subsection{Postulates}
% \input{pgfs/Postulates/user.tex}
% \\
% \input{pgfs/Postulates/system.tex}
% \\
% \input{pgfs/Postulates/rss.tex}

% \subsection{RecordParameters}
% \input{pgfs/RecordParameters/user.tex}
% \\
% \input{pgfs/RecordParameters/system.tex}
% \\
% \input{pgfs/RecordParameters/rss.tex}

% \subsection{SequentialDefinitions}
% \input{pgfs/SequentialDefinitions/user.tex}
% \\
% \input{pgfs/SequentialDefinitions/system.tex}
% \\
% \input{pgfs/SequentialDefinitions/rss.tex}

% \subsection{SequentialDependentRecords}
% \input{pgfs/SequentialDependentRecords/user.tex}
% \\
% \input{pgfs/SequentialDependentRecords/system.tex}
% \\
% \input{pgfs/SequentialDependentRecords/rss.tex}

% \subsection{SequentialSimpleRecords}
% \input{pgfs/SequentialSimpleRecords/user.tex}
% \\
% \input{pgfs/SequentialSimpleRecords/system.tex}
% \\
% \input{pgfs/SequentialSimpleRecords/rss.tex}

% \subsection{SimpleDataDefinitions}
% \input{pgfs/SimpleDataDefinitions/user.tex}
% \\
% \input{pgfs/SimpleDataDefinitions/system.tex}
% \\
% \input{pgfs/SimpleDataDefinitions/rss.tex}

\end{document}

